
    def extract_raster_file(ds, glob_filename):
        fs = ds.filesystem()

        files = list(fs.ls(glob=glob_filename))
        print(files)
        fname = files[0][0]
        print(fname)

        tmp = tempfile.NamedTemporaryFile()
        with fs.open(fname, "rb") as f:
            shutil.copyfileobj(f, tmp)
            tmp.flush()

        return tmp
#  rasterfile_raw = u.extract_raster_file(ds_raw, rastername)

    raster_file_veg = extract_raster_file(dead_tree_img, 'Dead_tree_canopy_cover_3m/California-Vegetation-DeadTreeCover-2018-fall-00003m.tif'
        )










def start():
    transforms = []
    raster_files = constants.RASTER_FILES

    for raster_file in raster_files:
        raster_file_output = constants.RASTER_OUTPUT_BASE_PATH.format(f"{raster_file}")
        raster_file_path = f"production/spatial_data/{raster_file}.tif.parquet"
        transforms = add_geometry_parquet_transforms(transforms, constants.RASTER_INPUT_SOURCE, raster_file_output, raster_file_path)
    
    return transforms


def add_geometry_parquet_transforms(transforms, fs_dataset, output_path, submodel_prediction_file_path):
    @configure_geospatial()
    @transform(
        out_dataset=Output(output_path),
        input_dataset=Input(fs_dataset)
    ) 
    def convert_to_dataset(input_dataset, out_dataset, ctx):
        fs = input_dataset.filesystem()
        hadoop_path = fs.hadoop_path
        file_list = list(fs.ls())
        paths = [hadoop_path + '/' + f.path for f in file_list]
        for i in paths:
            if submodel_prediction_file_path in i:
               fileT = [i, ]
               df = ctx.spark_session.read.parquet(*fileT)
               SedonaRegistrator.registerAll(ctx.spark_session)
               df = df.withColumn('geometry', ST.geom_from_wkt('geometry_wkt'))
               df = df.withColumn('geometry', ST.as_geo_json('geometry'))                
               out_dataset.write_dataframe(df)

    transforms.append(convert_to_dataset)
    return transforms

