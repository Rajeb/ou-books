# Add the top 10 minimum distances to the event DataFrame
for i in range(10):
    event_df[f'top_{i+1}_min_distance'] = top_10_distances[:, i]

# Save the updated event DataFrame with top 10 minimum distances
event_df.to_csv('event_top_10_min_distances.csv', index=False)
print("Top 10 distances saved to 'event_top_10_min_distances.csv'")











import pandas as pd
import numpy as np
import os
from scipy.spatial import cKDTree

# Load event locations
event_df = pd.read_csv('event_locations.csv')  # Contains lat, long for event locations
event_coords = event_df[['lat', 'long']].to_numpy()

# Checkpoint file path
checkpoint_file = 'min_distances_checkpoint.csv'

# Check if checkpoint exists
if os.path.exists(checkpoint_file):
    # Load the checkpointed minimum distances
    print(f"Resuming from checkpoint: {checkpoint_file}")
    checkpoint_df = pd.read_csv(checkpoint_file)
    processed_chunks = checkpoint_df['chunk_index'].max() + 1
    min_distances = checkpoint_df['min_distance'].to_numpy()
else:
    # Initialize for the first run
    print("No checkpoint found, starting fresh")
    processed_chunks = 0
    min_distances = np.full(len(event_coords), np.inf)  # Initialize with infinity (no distances yet)

# Function to calculate minimum distance between events and water points
def calculate_min_distance(event_coords, water_df_chunk):
    water_coords = water_df_chunk[['lat', 'long']].to_numpy()
    
    # Build a spatial index (KDTree) for the water points
    tree = cKDTree(water_coords)
    
    # Query the tree for the nearest water point for each event
    distances, indices = tree.query(event_coords)
    
    return distances

# Process large water occurrence data in chunks
water_files = ['water1.csv', 'water2.csv', 'water3.csv']
chunk_size = 100000  # Adjust based on available memory
chunk_index = 0

for water_file in water_files:
    print(f"Processing water file: {water_file}")
    
    for water_chunk in pd.read_csv(water_file, chunksize=chunk_size):
        if chunk_index < processed_chunks:
            # Skip the already processed chunks
            chunk_index += 1
            continue
        
        print(f"Processing chunk {chunk_index}")
        
        # Calculate the minimum distance between each event and the current chunk of water points
        distances = calculate_min_distance(event_coords, water_chunk)
        
        # Update minimum distances (keeping the smallest distance for each event)
        min_distances = np.minimum(min_distances, distances)
        
        # Save intermediate results as a checkpoint
        checkpoint_df = pd.DataFrame({
            'chunk_index': [chunk_index] * len(event_coords),
            'event_lat': event_df['lat'],
            'event_long': event_df['long'],
            'min_distance': min_distances
        })
        checkpoint_df.to_csv(checkpoint_file, index=False)
        
        print(f"Checkpoint saved at chunk {chunk_index}, first 5 distances: {min_distances[:5]}")
        
        # Move to the next chunk
        chunk_index += 1

# Add the minimum distance to the event DataFrame
event_df['min_distance_to_water'] = min_distances

# Output the event DataFrame with the minimum distance column
final_output_file = 'event_min_distances.csv'
event_df.to_csv(final_output_file, index=False)
print(f"Final output saved to {final_output_file}")












import pandas as pd
import numpy as np
from scipy.spatial import cKDTree

# Load event locations
event_df = pd.read_csv('event_locations.csv')  # Contains lat, long for event locations
event_coords = event_df[['lat', 'long']].to_numpy()

# Function to calculate minimum distance between events and water points
def calculate_min_distance(event_coords, water_df_chunk):
    water_coords = water_df_chunk[['lat', 'long']].to_numpy()
    
    # Build a spatial index (KDTree) for the water points
    tree = cKDTree(water_coords)
    
    # Query the tree for the nearest water point for each event
    distances, indices = tree.query(event_coords)
    
    # Return the minimum distance for each event
    return distances

# Process large water occurrence data in chunks
water_files = ['water1.csv', 'water2.csv', 'water3.csv']
min_distances = []

for water_file in water_files:
    print(f"Processing water file: {water_file}")
    
    # Load water occurrence data in chunks
    chunk_size = 100000  # Adjust based on available memory
    for water_chunk in pd.read_csv(water_file, chunksize=chunk_size):
        # Calculate the minimum distance between each event and the current chunk of water points
        distances = calculate_min_distance(event_coords, water_chunk)
        min_distances.append(distances)

# Combine results across all water chunks
final_distances = np.min(np.vstack(min_distances), axis=0)

# Add the minimum distance to the event DataFrame
event_df['min_distance_to_water'] = final_distances

# Output the event DataFrame with the minimum distance column
print(event_df.head())



















import os
import pandas as pd
import dask.dataframe as dd
import geopandas as gpd
from shapely.geometry import Point
from geopy.distance import great_circle
from dask import delayed

# Constants
BUFFER_DISTANCES_MILES = list(range(10, 110, 10))  # Buffers from 10 to 100 miles (increment by 10)
csv_directory = 'path_to_directory_with_csv_files'
csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]

# Initialize buffer results
buffer_results = {buffer: 0 for buffer in BUFFER_DISTANCES_MILES}

# Create a log file to track progress
log_file_path = 'progress_log.txt'
with open(log_file_path, 'w') as log_file:
    log_file.write('Starting processing\n')

# Function to load and process CSV file
@delayed
def load_and_process_csv(file_path):
    log_message = f'Loading {file_path}\n'
    with open(log_file_path, 'a') as log_file:
        log_file.write(log_message)
        
    dask_df = dd.read_csv(file_path)
    dask_df = dask_df[(dask_df['value'] != 0) & (dask_df['value'] != -9999)]
    return dask_df

# Convert to GeoDataFrame and create spatial index
def create_geodataframe(df):
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))
    gdf.set_crs('EPSG:4326', inplace=True)  # WGS84
    return gdf

# Function to calculate minimum distance
def calculate_event_distances(event_lat, event_lon, gdf, buffer_distance_miles):
    event_point = Point(event_lon, event_lat)
    buffer = event_point.buffer(buffer_distance_miles / 3963.2)  # 3963.2 miles is the Earth's radius
    buffer_gdf = gdf[gdf.geometry.intersects(buffer)]
    if buffer_gdf.empty:
        return 0
    return len(buffer_gdf)

# Load events
df_events = pd.DataFrame({
    'event_id': [1, 2, 3],  # Example event IDs
    'latitude': [34.1, 35.2, 36.5],
    'longitude': [-118.5, -119.6, -120.7]
})

# Convert events to GeoDataFrame
gdf_events = gpd.GeoDataFrame(df_events, geometry=gpd.points_from_xy(df_events['longitude'], df_events['latitude']))
gdf_events.set_crs('EPSG:4326', inplace=True)  # WGS84

# Process each CSV file (water occurrence data)
for csv_file in csv_files:
    file_path = os.path.join(csv_directory, csv_file)
    
    # Load and process CSV file
    dask_df = load_and_process_csv(file_path).compute()
    gdf = create_geodataframe(dask_df)  # Convert to GeoDataFrame

    # Progress log
    with open(log_file_path, 'a') as log_file:
        log_file.write(f'Processing file {file_path}\n')

    # Loop through each buffer distance
    for buffer_distance in BUFFER_DISTANCES_MILES:
        log_message = f'Processing buffer distance {buffer_distance} miles\n'
        with open(log_file_path, 'a') as log_file:
            log_file.write(log_message)
        
        # Initialize a set to keep track of events with water occurrence within the buffer
        events_with_water = set()
        
        # Loop through each event
        for event_idx, event_row in df_events.iterrows():
            event_lat, event_lon = event_row['latitude'], event_row['longitude']
            
            # Calculate distances
            count = calculate_event_distances(event_lat, event_lon, gdf, buffer_distance)
            if count > 0:
                events_with_water.add(event_row['event_id'])
        
        # Count how many events have at least one water surface within the current buffer distance
        buffer_results[buffer_distance] += len(events_with_water)
        
        # Log intermediate results
        with open(log_file_path, 'a') as log_file:
            log_file.write(f'{len(events_with_water)} events have at least one water surface within {buffer_distance} miles\n')

# Final results
with open(log_file_path, 'a') as log_file:
    log_file.write('Final results:\n')
    for buffer_distance, count in buffer_results.items():
        log_file.write(f'{count} events have at least one water surface within {buffer_distance} miles\n')
    log_file.write('Processing completed\n')

# Display final results
for buffer_distance, count in buffer_results.items():
    print(f"{count} events have at least one water surface within {buffer_distance} miles")








import os
import pandas as pd
import dask.dataframe as dd
import geopandas as gpd
from shapely.geometry import Point
from geopy.distance import great_circle
from dask import delayed

# Constants
BUFFER_DISTANCES_MILES = list(range(10, 110, 10))  # Buffers from 10 to 100 miles (increment by 10)
csv_directory = 'path_to_directory_with_csv_files'
csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]

# Initialize buffer results
buffer_results = {buffer: 0 for buffer in BUFFER_DISTANCES_MILES}

# Create a log file to track progress
log_file_path = 'progress_log.txt'
with open(log_file_path, 'w') as log_file:
    log_file.write('Starting processing\n')

# Function to load and process CSV file
@delayed
def load_and_process_csv(file_path):
    log_message = f'Loading {file_path}\n'
    with open(log_file_path, 'a') as log_file:
        log_file.write(log_message)
        
    dask_df = dd.read_csv(file_path)
    dask_df = dask_df[(dask_df['value'] != 0) & (dask_df['value'] != -9999)]
    return dask_df

# Convert to GeoDataFrame and create spatial index
def create_geodataframe(df):
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))
    gdf.set_crs('EPSG:4326', inplace=True)  # WGS84
    return gdf

# Function to calculate minimum distance
def calculate_event_distances(event_lat, event_lon, gdf, buffer_distance_miles):
    event_point = Point(event_lon, event_lat)
    buffer = event_point.buffer(buffer_distance_miles / 3963.2)  # 3963.2 miles is the Earth's radius
    buffer_gdf = gdf[gdf.geometry.intersects(buffer)]
    if buffer_gdf.empty:
        return 0
    return len(buffer_gdf)

# Load events
df_events = pd.DataFrame({
    'event_id': [1, 2, 3],  # Example event IDs
    'latitude': [34.1, 35.2, 36.5],
    'longitude': [-118.5, -119.6, -120.7]
})

# Convert events to GeoDataFrame
gdf_events = gpd.GeoDataFrame(df_events, geometry=gpd.points_from_xy(df_events['longitude'], df_events['latitude']))
gdf_events.set_crs('EPSG:4326', inplace=True)  # WGS84

# Process each CSV file (water occurrence data)
for csv_file in csv_files:
    file_path = os.path.join(csv_directory, csv_file)
    
    # Load and process CSV file
    dask_df = load_and_process_csv(file_path).compute()
    gdf = create_geodataframe(dask_df)  # Convert to GeoDataFrame

    # Progress log
    with open(log_file_path, 'a') as log_file:
        log_file.write(f'Processing file {file_path}\n')

    # Loop through each buffer distance
    for buffer_distance in BUFFER_DISTANCES_MILES:
        log_message = f'Processing buffer distance {buffer_distance} miles\n'
        with open(log_file_path, 'a') as log_file:
            log_file.write(log_message)
        
        # Initialize a set to keep track of events with water occurrence within the buffer
        events_with_water = set()
        
        # Loop through each event
        for event_idx, event_row in df_events.iterrows():
            event_lat, event_lon = event_row['latitude'], event_row['longitude']
            
            # Calculate distances
            count = calculate_event_distances(event_lat, event_lon, gdf, buffer_distance)
            if count > 0:
                events_with_water.add(event_row['event_id'])
        
        # Count how many events have at least one water surface within the current buffer distance
        buffer_results[buffer_distance] += len(events_with_water)
        
        # Log intermediate results
        with open(log_file_path, 'a') as log_file:
            log_file.write(f'{len(events_with_water)} events have at least one water surface within {buffer_distance} miles\n')

# Final results
with open(log_file_path, 'a') as log_file:
    log_file.write('Final results:\n')
    for buffer_distance, count in buffer_results.items():
        log_file.write(f'{count} events have at least one water surface within {buffer_distance} miles\n')
    log_file.write('Processing completed\n')

# Display final results
for buffer_distance, count in buffer_results.items():
    print(f"{count} events have at least one water surface within {buffer_distance} miles")


# Display results
for buffer_distance, count in buffer_results.items():
    print(f"{count} events have at least one water surface within {buffer_distance} miles")












import os
import pandas as pd

# Define the directory containing CSV files
csv_directory = 'path_to_directory_with_csv_files'

# List all CSV files in the directory
csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]

# Initialize a list to store the dataframes
dataframes = []

# Set a chunk size to process the data in smaller parts (e.g., 10000 rows at a time)
chunk_size = 10000

# Loop through each CSV file
for csv_file in csv_files:
    file_path = os.path.join(csv_directory, csv_file)
    
    # Initialize chunk counter and total progress tracking
    chunk_counter = 0

    # Read the CSV file in chunks
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # Filter out rows with values of 0 or no-data
        chunk = chunk[(chunk['value'] != 0) & (chunk['value'] != -9999)]
        
        # Process the chunk or append it to the list
        dataframes.append(chunk)
        
        # Update chunk counter
        chunk_counter += 1

        # Print progress as number of chunks processed
        print(f"Processed {chunk_counter * chunk_size} rows from {csv_file}")
        
    print(f"Completed processing {csv_file}")

# After processing all chunks, you can concatenate the dataframes if necessary
df_combined = pd.concat(dataframes, ignore_index=True)









# Initialize a list to store the dataframes
dataframes = []

# Set a chunk size to process the data in smaller parts (e.g., 10000 rows at a time)
chunk_size = 10000

for csv_file in csv_files:
    file_path = os.path.join(csv_directory, csv_file)
    
    # Read the CSV file in chunks
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # Filter out rows with values of 0 or no-data
        chunk = chunk[(chunk['value'] != 0) & (chunk['value'] != -9999)]
        
        # Process the chunk or append it to the list (use append only if required)
        dataframes.append(chunk)

# After processing all chunks, you can concatenate the dataframes if necessary
df_combined = pd.concat(dataframes, ignore_index=True)
############ Calculate miimum distance #############
csv_directory = 'path_to_directory_with_csv_files'
csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]

# Prepare a list of DataFrames by reading each CSV file
dataframes = []
for csv_file in csv_files:
    file_path = os.path.join(csv_directory, csv_file)
    df = pd.read_csv(file_path)
    dataframes.append(df)

# Define a function to process water bodies and calculate minimum distance
def process_water_bodies_in_chunks(water_df, df_events, chunk_size=10000):
    min_distances = []

    # Iterate through the water body DataFrame in chunks
    for start in range(0, len(water_df), chunk_size):
        chunk = water_df.iloc[start:start+chunk_size]
        
        # Filter out zero values or no-data values
        chunk = chunk[(chunk['value'] != 0) & (chunk['value'] != -9999)]
        
        # Build KDTree from the chunk's water body locations
        water_tree = KDTree(chunk[['latitude', 'longitude']])
        
        # Calculate minimum distance for each event in df_events
        distances, _ = water_tree.query(df_events[['latitude', 'longitude']], k=1)
        
        # Store the minimum distances
        min_distances.append(distances)
    
    # Combine all minimum distances from chunks
    return np.min(min_distances, axis=0)

# Initialize checkpoint and final results DataFrame
checkpoint_file = 'min_distances_checkpoint.csv'
if os.path.exists(checkpoint_file):
    df_events = pd.read_csv(checkpoint_file)
    print("Checkpoint loaded.")
else:
    for df_water in dataframes:
        df_water.columns = ['latitude', 'longitude', 'value']  # Adjust as needed
    df_events['min_distance_overall'] = np.nan

# Track progress and calculate distances
total_events = len(df_events)
for idx, event in df_events.iterrows():
    if not pd.isna(event['min_distance_overall']):
        continue
    
    for df_water in dataframes:
        df_water.columns = ['latitude', 'longitude', 'value']  # Adjust as needed
        min_distances = process_water_bodies_in_chunks(df_water, df_events.iloc[[idx]])
        df_events.at[idx, f'min_distance_{df_water.index.name}'] = min_distances[0]
    
    df_events.at[idx, 'min_distance_overall'] = df_events[[f'min_distance_{df_water.index.name}' for df_water in dataframes]].min(axis=1)
    df_events.to_csv(checkpoint_file, index=False)
    
    progress_percentage = (idx + 1) / total_events * 100
    print(f"Processed event {idx + 1}/{total_events} ({progress_percentage:.2f}%)")

df_events.to_csv('final_min_distances.csv', index=False)
print("Processing complete. Final results saved to 'final_min_distances.csv'.")







################################################################

import pandas as pd
import numpy as np
from scipy.spatial import KDTree
import os

# Load event locations into a DataFrame
df_events = pd.DataFrame({
    'latitude': [34.1, 35.2, 36.3, 37.4],  # Example latitudes (add your actual data)
    'longitude': [-118.5, -119.6, -120.7, -121.8]  # Example longitudes (add your actual data)
})

# Define water body CSV files
water_files = ['water_bodies1.csv', 'water_bodies2.csv', 'water_bodies3.csv']

# Define checkpoint file
checkpoint_file = 'min_distances_checkpoint.csv'

# Load checkpoint if it exists
if os.path.exists(checkpoint_file):
    df_events = pd.read_csv(checkpoint_file)
    print("Checkpoint loaded.")
else:
    # Initialize columns for each water file
    for water_file in water_files:
        df_events[f'min_distance_{water_file}'] = np.nan
    df_events['min_distance_overall'] = np.nan

# Function to process water body CSV in chunks and calculate minimum distance
def process_water_bodies_in_chunks(water_file, df_events, chunk_size=10000):
    min_distances = []

    # Iterate through the water body CSV file in chunks
    for chunk in pd.read_csv(water_file, chunksize=chunk_size):
        # Filter out zero values or no-data values (assuming no-data is -9999 or adjust accordingly)
        chunk = chunk[(chunk['value'] != 0) & (chunk['value'] != -9999)]
        
        # Build KDTree from the chunk's water body locations
        water_tree = KDTree(chunk[['latitude', 'longitude']])
        
        # Calculate minimum distance for each event in df_events
        distances, indices = water_tree.query(df_events[['latitude', 'longitude']], k=1)
        
        # Store the minimum distances
        min_distances.append(distances)

    # Combine all minimum distances from chunks
    return np.min(min_distances, axis=0)

# Calculate the minimum distances for each event from all water files
total_events = len(df_events)
for idx, event in df_events.iterrows():
    # Skip if this event has already been processed (i.e., min_distance_overall is not NaN)
    if not pd.isna(event['min_distance_overall']):
        continue

    # Process the event for each water file
    for water_file in water_files:
        min_distances = process_water_bodies_in_chunks(water_file, df_events.iloc[[idx]])  # Calculate for one event
        df_events.at[idx, f'min_distance_{water_file}'] = min_distances[0]
    
    # After processing all water files, find the overall minimum distance for the event
    df_events.at[idx, 'min_distance_overall'] = df_events.loc[idx, [f'min_distance_{file}' for file in water_files]].min()
    
    # Save the DataFrame to the checkpoint file after each event
    df_events.to_csv(checkpoint_file, index=False)
    
    # Calculate and print the percentage progress
    progress_percentage = (idx + 1) / total_events * 100
    print(f"Processed event {idx + 1}/{total_events} ({progress_percentage:.2f}%)")

# After processing all events, the final results are saved
df_events.to_csv('final_min_distances.csv', index=False)
print("Processing complete. Final results saved to 'final_min_distances.csv'.")















import rasterio
import numpy as np
import pandas as pd
from scipy.spatial import KDTree

def get_raster_df_chunk(raster_path, window, no_data_value):
    """
    Read a chunk of the raster data and return a DataFrame with valid pixel coordinates and values.
    :param raster_path: Path to the raster file
    :param window: Tuple defining the chunk to read (row_start, row_stop, col_start, col_stop)
    :param no_data_value: No-data value to filter out
    :return: DataFrame with raster coordinates and values
    """
    with rasterio.open(raster_path) as src:
        # Read the chunk of the raster data
        raster_data = src.read(1, window=window)
        
        # Get the coordinates of the pixels within the chunk
        rows, cols = np.where(raster_data != no_data_value)
        raster_values = raster_data[rows, cols]
        coords = [src.xy(row + window[0], col + window[2]) for row, col in zip(rows, cols)]
        
    return pd.DataFrame({'latitude': [c[1] for c in coords], 'longitude': [c[0] for c in coords], 'value': raster_values})

def process_raster_in_chunks(raster_path, chunk_size=512):
    """
    Process the raster file in chunks and return a DataFrame with all valid pixel coordinates and values.
    :param raster_path: Path to the raster file
    :param chunk_size: Size of each chunk
    :return: DataFrame with all valid pixel coordinates and values
    """
    with rasterio.open(raster_path) as src:
        height, width = src.height, src.width
        no_data_value = src.nodata
        
        # Initialize an empty list to collect DataFrames
        all_chunks_df = []
        
        # Iterate over chunks of the raster
        for row_start in range(0, height, chunk_size):
            row_stop = min(row_start + chunk_size, height)
            for col_start in range(0, width, chunk_size):
                col_stop = min(col_start + chunk_size, width)
                window = ((row_start, row_stop), (col_start, col_stop))
                
                # Read and process the chunk
                chunk_df = get_raster_df_chunk(raster_path, window, no_data_value)
                all_chunks_df.append(chunk_df)
        
        # Combine all chunks into a single DataFrame
        return pd.concat(all_chunks_df, ignore_index=True)

# Example usage:
raster_path = 'large_raster.tif'
chunk_size = 512  # Adjust chunk size based on available memory

# Process raster file in chunks
raster_df = process_raster_in_chunks(raster_path, chunk_size)

# Prepare your DataFrame with event locations
df_events = pd.DataFrame({
    'latitude': [35.0, 34.5],  # Example latitudes
    'longitude': [-120.5, -121.0]  # Example longitudes
})

# Function to calculate minimum distance using KDTree
def calculate_min_distance_kdtree(lat, lon, raster_df):
    tree = KDTree(raster_df[['latitude', 'longitude']])
    event_coords = np.array([lat, lon])
    _, indices = tree.query(event_coords, k=1)
    min_distance = np.linalg.norm(event_coords - raster_df.iloc[indices][['latitude', 'longitude']].values)
    return min_distance

# Calculate minimum distances for each event
for idx, row in df_events.iterrows():
    lat, lon = row['latitude'], row['longitude']
    min_distance = calculate_min_distance_kdtree(lat, lon, raster_df)
    df_events.loc[idx, 'min_distance'] = min_distance

print(df_events)












--------------------------------------------------------------

t pandas as pd
from scipy.spatial import distance

# Function to filter raster based on a bounding box around the event location
def get_nearby_raster_values(lat, lon, src, buffer_distance=0.1):
    """
    Returns the raster values and their coordinates within a buffer around the given lat, lon.
    :param lat: Latitude of the event
    :param lon: Longitude of the event
    :param src: Opened rasterio object
    :param buffer_distance: Buffer distance (degrees) around the event location
    :return: DataFrame with nearby raster values and coordinates
    """
    # Get the raster's bounds in world coordinates
    minx, miny, maxx, maxy = src.bounds

    # Define the bounding box around the event
    left = lon - buffer_distance
    right = lon + buffer_distance
    bottom = lat - buffer_distance
    top = lat + buffer_distance

    # Ensure the bounding box does not exceed raster bounds
    left = max(left, minx)
    right = min(right, maxx)
    bottom = max(bottom, miny)
    top = min(top, maxy)

    # Get the pixel window that covers the bounding box
    row_start, col_start = src.index(left, top)
    row_stop, col_stop = src.index(right, bottom)

    # Read the subset of the raster data within the bounding box
    window = ((row_start, row_stop), (col_start, col_stop))
    raster_subset = src.read(1, window=window)

    # Get the coordinates of the pixels within the window
    rows, cols = np.where(raster_subset != src.nodata)
    raster_values = raster_subset[rows, cols]
    coords = [src.xy(row + row_start, col + col_start) for row, col in zip(rows, cols)]

    # Create a DataFrame with the coordinates and values
    raster_df = pd.DataFrame(coords, columns=['latitude', 'longitude'])
    raster_df['value'] = raster_values

    return raster_df













# Step 1: Function to extract raster data and return it as a DataFrame
def raster_to_dataframe(raster_path):
    with rasterio.open(raster_path) as src:
        raster_data = src.read(1)  # Reading the first band of the raster
        no_data_value = src.nodata
        transform = src.transform

        # Generate arrays of pixel coordinates (row, col) and filter no-data values
        rows, cols = np.where(raster_data != no_data_value)
        raster_values = raster_data[rows, cols]

        # Convert pixel (row, col) to lat/long using the transform
        lat_long_coords = np.array([src.transform * (col, row) for row, col in zip(rows, cols)])
        lats, lons = zip(*lat_long_coords)

        # Create a DataFrame with lat, lon, and raster values
        df_raster = pd.DataFrame({
            'latitude': lats,
            'longitude': lons,
            'raster_value': raster_values
        })

    return df_raster

# Step 2: Prepare your list of raster files
raster_files = ['raster1.tif', 'raster2.tif', 'raster3.tif']

# Step 3: Convert each raster file to a DataFrame and store it in a list
raster_dfs = []
for raster_path in raster_files:
    df_raster = raster_to_dataframe(raster_path)
    raster_dfs.append(df_raster)

# Step 4: Print the first few rows of one of the raster DataFrames
print(raster_dfs[0].head())  # This will print the first few rows of the first raster's DataFrame
