

def create_geometry(df, geometry_field):
    df = df.withColumn("geom", ST_GeomFromGeoJSON(geometry_field))
    df = df.withColumn("valid", ST_IsValid("geom"))
    df = df.filter(col("valid") == True).drop("valid")
    df = df.withColumn(
        "geom", ST_Transform("geom", lit("EPSG:4326"), lit("EPSG:32610"))
    )
    df = df.withColumn("geometry", ST_AsText("geom"))
    return df




def link_function(ctx, failures, fia_geometry_cleaned):
    # create geometries
    failures = create_geometry(failures, "geometry")
    fia_geometry_cleaned = create_geometry(fia_geometry_cleaned, "geometry")

    # # do intersections
    failures.registerTempTable("failures")
    fia_geometry_cleaned.registerTempTable("fia_geometry_cleaned")

    queryOverlap = """
        SELECT p.*, z.fia
    #     FROM failures as p, fia_geometry_cleaned as z
    #     WHERE ST_Intersects(p.geom, z.geom)
return df

    intersection_dataframe = ctx.spark_session.sql(queryOverlap)  # noqa

    return intersection_dataframe.drop("geom")
    
    # SELECT p.*, z.fia
    #     FROM failures as p, fia_geometry_cleaned as z
    #     WHERE ST_Intersects(p.geom, z.geom)
return df


def get_pixel_primary_key(
    spark,
    point_df,
    pixel_df,
    point_df_primary_keys: list = ["global_id"],
    point_df_geom_col: str = "geometry",
    pixel_primary_key: str = "pk_row_col",
    pixel_geom_col: str = "geometry",
    include_all_pixel_columns=False,
):
    """
    Label transformers with the pixel primary key so that spatial & weather data can be joined via pixel
    """
    # Concatenate the primary keys into a single column called 'combined_pk'
    combined_txfr_pk = F.concat_ws("_", *point_df_primary_keys).alias("combined_pk")

    # filter to temp dataset to avoid error from Null geometries
    df_temp = (
        point_df.filter(F.col(point_df_geom_col).isNotNull())
        .select(
            combined_txfr_pk,
            stc.ST_GeomFromGeoJSON(point_df_geom_col).alias(
                point_df_geom_col
            ),  # convert to geometry type
        )
        .distinct()
    )

    # prep the pixel df
    pixel_df = (
        pixel_df.filter(F.col(pixel_geom_col).isNotNull())
        .select(
            pixel_primary_key,
            stc.ST_GeomFromGeoJSON(pixel_geom_col).alias("pixel_geometry"),
        )
        .distinct()
    )

    # join with txfr with pixel dataset using indexed & optimized spatial join function
    df_temp = polygon_contains_point(
        spark=spark,
        df_polygons=pixel_df,
        df_points=df_temp,
        polygon_geometry="pixel_geometry",
        point_geometry=point_df_geom_col,
        max_partitions=64,
    )















Ambiguous self-joins fail in Spark 3

Message not helpful?
Column date#76 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via Dataset.alias before joining them, and specify the column using the qualified name, e.g. df.alias("a").join(df.alias("b"), F.col("a.id") > F.col("b.id")). Please see documentation for more details.

url ="https://yb8oqn9m7d.internal.us-west-2.ss.pge.com/rating_on_fia_startdate/v1?start_date=20240715"

except requests.exceptions.RequestException as e:
    print(f"Error encountered: {e}")
url = “https://yb8oqn9m7d.internal.us-west-2.ss.pge.com/rating_on_fia_startdate/v1”
api_key = " ciMlnk2Bmp3rMDXtItRw24hVKPLySKmq8asLBYEb "
header = {"x-api-key": api_key}
response = requests.get(url=url, headers=header, verify=False)
fia_data = response.json()
print (fia_data)

############Climate data
def create_geometry(df, geometry_field):
    df = df.withColumn("geom", ST_GeomFromGeoJSON(geometry_field))
    df = df.withColumn("valid", ST_IsValid("geom"))
    df = df.filter(col("valid") == True).drop("valid")
    df = df.withColumn(
        "geom", ST_Transform("geom", lit("EPSG:4326"), lit("EPSG:32610"))
    )
    df = df.withColumn("geometry", ST_AsText("geom"))
    return df

def create_geometry(df, geometry_field):
    # Convert GeoJSON strings to geometries
    df['geometry'] = df[geometry_field].apply(lambda x: shape(x))
    
    # Create a GeoDataFrame
    gdf = gpd.GeoDataFrame(df, geometry='geometry')
    
    # Set the original CRS
    gdf.set_crs(epsg=4326, inplace=True)
    
    # Filter valid geometries
    gdf = gdf[gdf.is_valid]
    
    # Transform to a new CRS (EPSG:32610)
    gdf = gdf.to_crs(epsg=32610)
    
    # Convert geometries to WKT
    gdf['geometry'] = gdf['geometry'].apply(lambda geom: geom.wkt)
    
    return pd.DataFrame(gdf)

https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020JD033180
https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023JD039136


queryOverlap = """
        SELECT p.TREEID,p.wkt,tm.geometry as tm_geom, tm.value as mort_per, tm.pk_row_col
        FROM canopy_data as cp, tree_mortality_df as tm
        ST_Intersection(cp.wkt, mt.tm_geom)
        WHERE (ST_Intersects(ST_GeomFromWKT(cp.wkt), ST_GeomFromWKT(mt.tm_geom))
        """
edited
queryOverlap = """
    SELECT 
        cp.TREEID, 
        cp.wkt, 
        tm.geometry as tm_geom, 
        tm.value as mort_per, 
        tm.pk_row_col
    FROM 
        canopy_data as cp, 
        tree_mortality_df as tm
    WHERE 
        ST_Intersects(ST_GeomFromWKT(cp.wkt), ST_GeomFromWKT(tm.geometry))
    """


----------
transform(
    out=Output(
        "/PG&E/workflow_rada_dev/logic/TxCFO_models/datasets/vegetation/data_pipeline/raster_datasets/test1"
    ),
    tree_mortality_df=Input("ri.foundry.main.dataset.04a3e876-bc72-4acd-8fe8-b0815a4422f2"),
    canopy_data=Input("ri.foundry.main.dataset.86a4958b-bc33-491d-babd-d613dc6991cf")
)
def compute(ctx, out, tree_mortality_df, canopy_data):
    spark = ctx.spark_session
    SedonaRegistrator.registerAll(spark)
    canopy_data.registerTempTable("canopy_data")
    tree_mortality_df.registerTempTable("tree_mortality_df")
 
    queryOverlap = """
    SELECT
        cp.TREEID,
        cp.wkt,
        tm.geometry as tm_geom,
        tm.value as mort_per,
        tm.pk_row_col,
        AVG(tm.value) as avg_mort_per,
        ST_Intersection(ST_GeomFromWKT(cp.wkt), ST_GeomFromWKT(tm.geometry)) as intersection
    FROM
        canopy_data as cp, 
        tree_mortality_df as tm
    WHERE
        ST_Intersects(ST_GeomFromWKT(cp.wkt), ST_GeomFromWKT(tm.geometry))
    GROUP BY
    cp.TREEID
    """
    result_df =spark.sql(queryOverlap)
    out.write_dataframe(result_df)
